{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd9d599e",
   "metadata": {},
   "source": [
    "#ques1\n",
    "\n",
    "\n",
    "#overfitting : when model gives good accuracy on training dataset but perform bad on test dataset is called overfitting. In overfitting low bias and high variance.\n",
    "#underfitting: when model doesnot work good on both training and test dataset.In underfitting high bias and high variance.\n",
    "#ways to mitigate the overfitting\n",
    "1.early stopping\n",
    "2.reduce model complexity\n",
    "3.Improving the quality of training data reduces overfitting by focusing on meaningful patterns, mitigate the risk of fitting the noise or irrelevant features.\n",
    "Increase the training data can improve the model’s ability to generalize to unseen data and reduce the likelihood of overfitting.\n",
    "\n",
    "#ways to mitigate the underfitting\n",
    "1.increase model complexity\n",
    "2.create more features using feature engineering\n",
    "3.Remove noise from the data.\n",
    "4.Increase the number of epochs or increase the duration of training to get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98331659",
   "metadata": {},
   "source": [
    "#ques2\n",
    "\n",
    "#We can solve the problem of overfitting by:\n",
    "\n",
    "1.Train with more data\n",
    "With the increase in the training data, the crucial features to be extracted become prominent.\n",
    "The model can recognize the relationship between the input attributes and the output variable.\n",
    "The only assumption in this method is that the data to be fed into the model should be clean; otherwise, \n",
    "it would worsen the problem of overfitting.\n",
    "\n",
    "2.Data augmentation\n",
    "An alternative method to training with more data is data augmentation, \n",
    "which is less expensive and safer than the previous method. \n",
    "Data augmentation makes a sample data look slightly different every time the model processes it. \n",
    "\n",
    "3.Addition of noise to the input data \n",
    "Another similar option as data augmentation is adding noise to the input and output data.\n",
    "Adding noise to the input makes the model stable without affecting data quality and privacy while adding noise to the output \n",
    "makes the data more diverse. Noise addition should be done in limit so that it does not make the data incorrect or too different.\n",
    "\n",
    "4.Feature selection\n",
    "Every model has several parameters or features depending upon the number of layers, number of neurons, etc.\n",
    "The model can detect many redundant features or features determinable from other features leading to unnecessary complexity.\n",
    "We very well know that the more complex the model, the higher the chances of the model to overfit. \n",
    "\n",
    "5.Cross-validation\n",
    "Cross-validation is a robust measure to prevent overfitting. The complete dataset is split into parts.\n",
    "In standard K-fold cross-validation, we need to partition the data into k folds.\n",
    "Then, we iteratively train the algorithm on k-1 folds while using the remaining holdout fold as the test set.\n",
    "This method allows us to tune the hyperparameters of the neural network or machine learning model and test it using completely unseen data. \n",
    "\n",
    "6.Simplify data\n",
    "Till now, we have come across model complexity to be one of the top reasons for overfitting.\n",
    "The data simplification method is used to reduce overfitting by decreasing the complexity of the model to make it simple enough that it does not overfit.\n",
    "Some of the procedures include pruning a decision tree, reducing the number of parameters in a neural network, and using dropout on a neutral network. \n",
    "\n",
    "7.Regularization\n",
    "If overfitting occurs when a model is too complex, reducing the number of features makes sense. Regularization methods like \n",
    "Lasso, L1 can be beneficial if we do not know which features to remove from our model. \n",
    "Regularization applies a \"penalty\" to the input parameters with the larger coefficients, which subsequently limits the model's variance. \n",
    "\n",
    "8.Ensembling\n",
    "It is a machine learning technique that combines several base models to produce one optimal predictive model.\n",
    "In Ensemble learning,  the predictions are aggregated to identify the most popular result.\n",
    "Well-known ensemble methods include bagging and boosting, which prevents overfitting as an ensemble model is made from the aggregation of multiple models.\n",
    "\n",
    "9.Early stopping\n",
    "This method aims to pause the model's training before memorizing noise and random fluctuations from the data.\n",
    "There can be a risk that the model stops training too soon, leading to underfitting. One has to come to an optimum time/iterations the model should train. \n",
    "\n",
    "10.Adding dropout layers\n",
    "Large weights in a neural network signify a more complex network. \n",
    "Probabilistically dropping out nodes in the network is a simple and effective method to prevent overfitting.\n",
    "In regularization, some number of layer outputs are randomly ignored or “dropped out” to reduce the complexity of the model. \n",
    "\n",
    "Our tip: If one has two models with almost equal performance, the only difference being that one model is more complex than the other, one should always go with the less complex model.\n",
    "    In data science, it's a thumb rule that one should always start with a less complex model and add complexity over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c84991",
   "metadata": {},
   "source": [
    "#ques3 \n",
    "\n",
    "#Underfitting becomes obvious when the model is too simple and cannot create a relationship between the input and the output.\n",
    "It is detected when the training error is very high and the model is unable to learn from the training data.\n",
    "High bias and low variance are the most common indicators of underfitting.\n",
    "\n",
    "Underfitting happens when:\n",
    "\n",
    "1.Unclean training data containing noise or outliers can be a reason for the model not being able to derive patterns from the dataset.\n",
    "2.The model has a high bias due to the inability to capture the relationship between the input examples and the target values. This usually happens in the case of varied datasets.\n",
    "3.The model is assumed to be too simple—for example, we train a linear model in complex scenarios.\n",
    "4.Incorrect hyperparameters tuning often leads to underfitting due to under-observing of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5972a19",
   "metadata": {},
   "source": [
    "#ques4\n",
    "\n",
    "#bias: The bias is known as the difference between the prediction of the values by the Machine Learning model and the correct value. Being high in biasing gives a large error in training as well as testing data. It recommended that an algorithm should always be low-biased to avoid the problem of underfitting. By high bias, the data predicted is in a straight line format, thus not fitting accurately in the data in the data set. Such fitting is known as the Underfitting of Data.\n",
    "This happens when the hypothesis is too simple or linear in nature\n",
    "\n",
    "#variance :The variability of model prediction for a given data point which tells us the spread of our data is called the variance of the model. The model with high variance has a very complex fit to the training data and thus is not able to fit accurately on the data which it hasn’t seen before. As a result, such models perform very well on training data but have high error rates on test data. When a model is high on variance, it is then said to as Overfitting of Data. Overfitting is fitting the training set accurately via complex curve and high order hypothesis but is not the solution as the error with unseen data is high.\n",
    "While training a data model variance should be kept low.\n",
    "\n",
    "#Bias-variance tradeoff: If the algorithm is too simple (hypothesis with linear equation) then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex (hypothesis with high degree equation) then it may be on high variance and low bias. In the latter condition, the new entries will not perform well. Well, there is something between both of these conditions,\n",
    "known as a Trade-off or Bias Variance Trade-off. This tradeoff in complexity is why there is a tradeoff between bias and variance.\n",
    "An algorithm can’t be more complex and less complex at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746a85b2",
   "metadata": {},
   "source": [
    "#ques5\n",
    "\n",
    "#we can check if the model is overfitting or underfitting by using learning curves\n",
    "\n",
    "#A plot of learning curves shows underfitting if:\n",
    "\n",
    "1.The training loss remains flat regardless of training.\n",
    "2.The training loss continues to decrease until the end of training.\n",
    "\n",
    "#A plot of learning curves shows overfitting if:\n",
    "\n",
    "1.The plot of training loss continues to decrease with experience.\n",
    "2.The plot of validation loss decreases to a point and begins increasing again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7493e75a",
   "metadata": {},
   "source": [
    "#ques6\n",
    "\n",
    "#example linear regression on non linear data set is underfitting in this case high bias high variance.\n",
    "#curve fitting all data points of training dataset causes overfitting in this low bias and high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87324fed",
   "metadata": {},
   "source": [
    "#ques7\n",
    "\n",
    "#regularization: One of the simplest yet effective ways to prevent overfitting is through regularization. Regularization adds a penalty term to the loss function, discouraging the model from assigning too much importance to any one feature. Two common forms of #regularization are L1 and L2 regularization.\n",
    "\n",
    "#L1 regularization adds the absolute values of the weights to the loss function. This encourages some weights to become exactly zero, effectively performing feature selection. It’s a handy tool when you suspect that only a subset of your features is essential.\n",
    "\n",
    "#L2 regularization, on the other hand, adds the square of the weights to the loss function. This tends to evenly distribute the importance across all features, reducing the magnitude of weights and preventing them from growing too large."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
