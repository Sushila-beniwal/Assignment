{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7721b929",
   "metadata": {},
   "source": [
    "#ques1\n",
    "\n",
    "#It is a scaling technique method in which data points are shifted and rescaled so that they end up in a range of 0 to 1. It is also known as min-max scaling.\n",
    "\n",
    "The formula for calculating normalized score:\n",
    "\n",
    "X new = (X — X min)/ (X max — X min)\n",
    "\n",
    "Here, Xmax and Xmin are the maximum and minimum values of the feature respectively.\n",
    "\n",
    "· If X=Xmin; then Xnew =0\n",
    "\n",
    "Since numerator will become Xmin –Xmin, which is nothing but 0.\n",
    "\n",
    "· If X=Xmax ; then Xnew =1\n",
    "\n",
    "In this case, both numerator and denominator will be equal and cancel each other to give us the value of Xnew =1.\n",
    "\n",
    "It’s too complicated, isn’t it? Let’s take an example and clear this out.\n",
    "\n",
    "In CAT (an aptitude test conducted by IIM’s for the selection in top B-schools in India)too many applications are received. So they can’t examine all candidates at the same time, therefore the exam is conducted in shifts or even on different days. In different shifts, the question paper set is different. Although the questions are set in such a way that the difficulty level of each shift remains the same but still there might be a possibility that difficulty level of shift varies. So it will be unfair to the candidates who got a difficult set of questions. To make it fair for all candidates the score of candidates is normalized.\n",
    "\n",
    "Let’s say the exam is conducted in two shifts shift A and shift B and questions in shift A were relatively easy as compared to that of shift B. Because questions were relatively easy in shift A maximum marks scored by a candidate out of 300 in shift A is 280 and minimum is 80 on the other hand maximum and minimum marks scored by a candidate in shift B is 250 and 50 respectively.\n",
    "\n",
    "So we cannot compare the score of a candidate who scored 150 in shift A to a candidate who scored the same in shift B.\n",
    "\n",
    "Hence we normalize the scores\n",
    "\n",
    "The normalized score of a candidate who scored 150 in shift A will be calculated as follows\n",
    "\n",
    "For simplicity sake let’s name it Xa\n",
    "\n",
    "Xa =150-Xmin/(Xmin-Xmax)\n",
    "\n",
    "Xmax = 280\n",
    "\n",
    "Xmin = 80\n",
    "\n",
    "Putting these values we get\n",
    "\n",
    "Xa= 150–80/(280–80)\n",
    "\n",
    "Xa =0.35\n",
    "\n",
    "The normalized score of a candidate who scored 150 in shift B will be\n",
    "\n",
    "For simplicity sake let’s name it Xb\n",
    "\n",
    "Xb=150-Xmin/(Xmin-Xmax)\n",
    "\n",
    "Xmax = 250\n",
    "\n",
    "Xmin = 50\n",
    "\n",
    "Putting these values we get\n",
    "\n",
    "Xb = 150–50/(250–50)\n",
    "\n",
    "Xb =0.5\n",
    "\n",
    "Here we can see Xb>Xa\n",
    "\n",
    "Total marks in CAT exam=300\n",
    "\n",
    "If you want to compare out of 300 then\n",
    "\n",
    "Xa *300 =105\n",
    "\n",
    "Xb *300 = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804fa6c0",
   "metadata": {},
   "source": [
    "#ques 2\n",
    "\n",
    "\"\"\" unit vector scaling: it is technique to scale down the input vector to a vector having magnitude unity(1) without changing it's direction.\n",
    "    \n",
    "    but in min_max scaling the input data is converted into range 0-1. not neccesarily all have  1 magnitude.\n",
    "    It is useful when dealing with features with hard boundaries.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51005a5",
   "metadata": {},
   "source": [
    "#ques3\n",
    "\n",
    "\"\"\"\n",
    "PCA:PCA helps us to identify patterns in data based on the correlation between features. In a nutshell, PCA aims to find the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions than the original one.\n",
    "\n",
    "If we use PCA for dimensionality reduction, we construct a d x k–dimensional transformation matrix W that allows us to map a sample vector x onto a new k–dimensional feature subspace that has fewer dimensions than the original d–dimensional feature space:\n",
    "\n",
    "\n",
    "As a result of transforming the original d-dimensional data onto this new k-dimensional subspace (typically k ≪ d), the first principal component will have the largest possible variance, and all consequent principal components will have the largest variance given the constraint that these components are uncorrelated (orthogonal) to the other principal components — even if the input features are correlated, the resulting principal components will be mutually orthogonal (uncorrelated).\n",
    "\n",
    "Note that the PCA directions are highly sensitive to data scaling, and we need to standardize the features prior to PCA if the features were measured on different scales and we want to assign equal importance to all features.\n",
    "\n",
    "Before looking at the PCA algorithm for dimensionality reduction in more detail, let’s summarize the approach in a few simple steps:\n",
    "\n",
    "1.Standardize the d-dimensional dataset.\n",
    "2.Construct the covariance matrix.\n",
    "3.Decompose the covariance matrix into its eigenvectors and eigenvalues.\n",
    "4.Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.\n",
    "5.Select k eigenvectors which correspond to the k largest eigenvalues, where k is the dimensionality of the new feature subspace (k ≤ d).\n",
    "6.Construct a projection matrix W from the “top” k eigenvectors.\n",
    "7.Transform the d-dimensional input dataset X using the projection matrix W to obtain the new k-dimensional feature subspace.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2718189",
   "metadata": {},
   "source": [
    "#QUES4\n",
    "\"\"\"\n",
    "Feature extraction: we keep the important fraction of all the features. We apply PCA to achieve this. \n",
    "PCA is a dimensionality reduction that identifies important relationships in our data, transforms the existing data based on these relationships, and then quantifies the importance of these relationships so we can keep the most important relationships and drop the others. To remember this definition, we can break it down into four steps:\n",
    "\n",
    "1.We identify the relationship among features through a Covariance Matrix.\n",
    "2.Through the linear transformation or eigendecomposition of the Covariance Matrix, we get eigenvectors and eigenvalues.\n",
    "3.Then we transform our data using Eigenvectors into principal components.\n",
    "4.Lastly, we quantify the importance of these relationships using Eigenvalues and keep the important principal components.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21fb8f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 5\n",
    "\n",
    "# import requred libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d92f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "data = pd.read_csv(\"link\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d51330d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import min_max scalre\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039c15f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data contains price,rating and delivery time\n",
    "#apply min-max scaling on all three column\n",
    "scaling =  MinMaxScaler()\n",
    "data[['price','rating','delivery time']] = scaling.fit_transform(data[['price','rating','delivery time']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c2a6cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 6\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "498f39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 7\n",
    "da ={'val':[1,5,10,15,20]}\n",
    "df = pd.DataFrame(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f5330cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4023c568",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling =  MinMaxScaler()\n",
    "df['scaled_val'] = scaling.fit_transform(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7e1addb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val</th>\n",
       "      <th>scaled_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>0.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   val  scaled_val\n",
       "0    1    0.000000\n",
       "1    5    0.210526\n",
       "2   10    0.473684\n",
       "3   15    0.736842\n",
       "4   20    1.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8280c97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
